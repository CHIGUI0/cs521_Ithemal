# Ithemal

**Ithemal** is a dataâ€‘driven neural model that estimates the *throughput* (CPU cycles) of x86â€‘64 basic blocks with high accuracy, portability, and speed.

> Mendis *etÂ al.* â€œIthemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks.â€ *ICMLÂ 2019*
> [\[arXiv\]](https://arxiv.org/abs/1808.07412)Â Â [\[BibTeX\]](http://groups.csail.mit.edu/commit/bibtex.cgi?key=ithemal-icml)

---

## QuickÂ Start

### 1Â Â·Â Install dependencies

```bash
# clone the repository (addÂ --recursive if you plan to retrain)
git clone https://github.com/YuxuanMa-sys/cs521_Ithemal.git
cd cs521_Ithemal

# create a dedicated env and install requirements
conda create -n ithemal --file env
conda activate ithemal
```

### 2Â Â·Â Configure environment

```bash
# point ITHEMAL_HOME to the repo root (edit the path as needed)
export ITHEMAL_HOME="/path/to/cs521_Ithemal"
```

### 3Â Â·Â Download data

Preâ€‘processed datasets for three microâ€‘architectures are available:

| ÂµArch      | File             | Size   |
| ---------- | ---------------- | ------ |
| Haswell    | `bhive_hsw.data` | 254Â MB |
| Ivy Bridge | `bhive_ivb.data` | 246Â GB |
| Skylake    | `bhive_skl.data` | 243Â GB |

Download the bundle from **GoogleÂ Drive** and extract it anywhere:

[ðŸ“¦Â IthemalÂ datasets](https://drive.google.com/file/d/1lr7k0Gomd2tHEvAw-jwRFFqnTcs4Rfg4/view?usp=sharing)

> **Need to regenerate?** Install LLVMÂ (â‰¥3.11) and run:
>
> ```bash
> bash bash_process_data.sh
> ```

---

## Preâ€‘trained Models & Prediction

Preâ€‘trained models matching the paper are hosted in a separate repository:

* **Haswell** â€“ [`paper/haswell/`](https://github.com/psg-mit/Ithemal-models/blob/master/paper/haswell)
* **Skylake** â€“ [`paper/skylake/`](https://github.com/psg-mit/Ithemal-models/blob/master/paper/skylake)
* **IvyÂ Bridge** â€“ [`paper/ivybridge/`](https://github.com/psg-mit/Ithemal-models/blob/master/paper/ivybridge)

```bash
python learning/pytorch/ithemal/predict.py \
    --model predictor.dump \
    --model-data trained.mdl \
    --file my_binary.o \
    --verbose
```

The script follows the same annotation convention as **IACA**. Example annotated binaries are under `learning/pytorch/examples/`.

---

## Training from Scratch

```bash
python learning/pytorch/ithemal/run_ithemal.py \
    --data ./processed_data/bhive_hsw.data \
    --use-rnn train \
    --experiment-name my_exp \
    --experiment-time $(date +%Y%m%d_%H%M%S) \
    --sgd --threads 4 --trainers 6 \
    --weird-lr --decay-lr --epochs 100
```

**Outputs** (under `learning/pytorch/saved/<EXPNAME>/<TIME>/`):

* `loss_report.log`Â â€“ tabâ€‘separated training progress
* `validation_results.txt`Â â€“ `predicted,actual` pairs for the test set
* `trained.mdl`Â â€“ learned weights
* `predictor.dump`Â â€“ architecture & vocab (use with `predict.py`)


or you can just run the provided script:

```bash
bash bash_train.sh
```
---

## RepositoryÂ Layout

```
learning/
â””â”€â”€ pytorch/
    â”œâ”€â”€ ithemal/         # CLI & training drivers
    â”œâ”€â”€ models/          # Model definitions & optimizers
    â”œâ”€â”€ data/            # Dataset loaders & batching
common/
â””â”€â”€ common_libs/
    â””â”€â”€ utilities.py     # Instruction & basicâ€‘block representation

data_collection/
â”œâ”€â”€ tokenizer/           # Canonicalization & tokenization (C)
â””â”€â”€ common/
```

---